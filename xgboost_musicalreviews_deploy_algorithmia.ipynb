{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with XGBoost on Algorithmia\n",
    "\n",
    "With this notebook, we will be training an XGBoost model on Amazon's Musical Instrument Reviews dataset and be able to use this model to predict the sentiment of the given texts. If you would like to see the final product first, you can check out this algorithm in action at https://algorithmia.com/algorithms/asli/xgboost_basic_sentiment_analysis\n",
    "\n",
    "## Overview\n",
    "Let's first go over the steps we will cover in this notebook. We will start with the end in mind and then slowly build up to that point. At the end of this demo, you will have an up and running on Algorithmia, ready to serve its predictions upon your requests!\n",
    "\n",
    "Step by step, we will: \n",
    "\n",
    "1. Create an algorithm on Algorithmia \n",
    "2. Clone the algorithm's repository on our local machine, so that we develop it locally \n",
    "3. Create the basic algorithm script and the dependencies file. We will code our script in advance, assuming that our model will be sitting on a remote path on Algorithmia and our script will load the model from there. We will then make these assumptions true!\n",
    "4. Commit and push these files to Algorithmia and get our Algorithm's container built\n",
    "5. Load the training data\n",
    "6. Preprocess the data\n",
    "7. Setup an XGBoost model and do a mini hyperparameter search\n",
    "8. Fit the data on our model\n",
    "9. Get the predictions\n",
    "10. Check the accuracy\n",
    "11. Repeat the steps through 6 and 10 until we are happy with our model :)\n",
    "11. Once we are happy, upload the to Algorithmia and have it up and ready to serve our upcoming prediction requests!\n",
    "12. Test our published algorithm with sample requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting up and ready on Algorithmia\n",
    "Let's first create an algorithm on Algorithmia and then build on it slowly.\n",
    "After importing the necessary packages, we'll define the variables to use across many of our calls to Algorithmia, through the Python API client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Algorithmia\n",
    "from Algorithmia.errors import AlgorithmException\n",
    "import urllib.parse\n",
    "from git import Git, Repo, remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"YOUR_API_KEY\"\n",
    "algo_client = Algorithmia.client(api_key)\n",
    "username = \"YOUR_USERNAME\"\n",
    "algo_name = \"xgboost_basic_sentiment_analysis\"\n",
    "algo_namespace = f\"{username}/{algo_name}\"\n",
    "\n",
    "local_dir = \"../algorithmia_repo\"\n",
    "algo_script_path = \"{}/{}/src/{}.py\".format(local_dir, algo_name, algo_name)\n",
    "dependency_file_path = \"{}/{}/{}\".format(local_dir, algo_name, \"requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the algorithm and cloning its repo\n",
    "You would only need to do this step once, because you only need one algorithm and cloning it once on your local environment is enough.\n",
    "\n",
    "Let's first define our functions to do these two things and then call them on the next step. Let's also have a utility class named Progress to see our Github framework's progress when operating on our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Progress(remote.RemoteProgress):\n",
    "    def line_dropped(self, line):\n",
    "        print(line)\n",
    "    def update(self, *args):\n",
    "        print(self._cur_line)\n",
    "\n",
    "p = Progress()\n",
    "\n",
    "def create_algorithm(algo_name):    \n",
    "    details = {\n",
    "        \"summary\": algo_name,\n",
    "        \"label\": algo_name,\n",
    "        \"tagline\": algo_name\n",
    "    }\n",
    "    settings = {\n",
    "        \"source_visibility\": \"closed\",\n",
    "        \"package_set\": \"python37\",\n",
    "        \"license\": \"apl\",\n",
    "        \"network_access\": \"full\",\n",
    "        \"pipeline_enabled\": True\n",
    "    }\n",
    "    algo_client.algo(algo_namespace).create(details, settings)\n",
    "    \n",
    "def clone_algorithm_repo():\n",
    "    # Encode API key, so we can use it in the git URL\n",
    "    encoded_api_key= urllib.parse.quote_plus(api_key)\n",
    "\n",
    "    algo_repo = \"https://{}:{}@git.algorithmia.com/git/{}/{}.git\".format(username, encoded_api_key, username, algo_name)\n",
    "    _ = Repo.clone_from(algo_repo, \"{}/{}\".format(local_dir, algo_name), progress=p)\n",
    "\n",
    "    cloned_repo = Repo(\"{}/{}\".format(local_dir, algo_name))\n",
    "    return cloned_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '../algorithmia_repo/xgboost_basic_sentiment_analysis'...\n",
      "POST git-upload-pack (157 bytes)\n",
      "remote: Counting objects: 1\n",
      "remote: Counting objects: 15, done\n",
      "remote: Finding sources:   6% (1/15)\n",
      "remote: Finding sources:  13% (2/15)\n",
      "remote: Finding sources:  20% (3/15)\n",
      "remote: Finding sources:  26% (4/15)\n",
      "remote: Finding sources:  33% (5/15)\n",
      "remote: Finding sources:  40% (6/15)\n",
      "remote: Finding sources:  46% (7/15)\n",
      "remote: Finding sources:  53% (8/15)\n",
      "remote: Finding sources:  60% (9/15)\n",
      "remote: Finding sources:  66% (10/15)\n",
      "remote: Finding sources:  73% (11/15)\n",
      "remote: Finding sources:  80% (12/15)\n",
      "remote: Finding sources:  86% (13/15)\n",
      "remote: Finding sources:  93% (14/15)\n",
      "remote: Finding sources: 100% (15/15)\n",
      "remote: Finding sources: 100% (15/15)\n",
      "remote: Getting sizes:   9% (1/11)\n",
      "remote: Getting sizes:  18% (2/11)\n",
      "remote: Getting sizes:  27% (3/11)\n",
      "remote: Getting sizes:  36% (4/11)\n",
      "remote: Getting sizes:  45% (5/11)\n",
      "remote: Getting sizes:  54% (6/11)\n",
      "remote: Getting sizes:  63% (7/11)\n",
      "remote: Getting sizes:  72% (8/11)\n",
      "remote: Getting sizes:  81% (9/11)\n",
      "remote: Getting sizes:  90% (10/11)\n",
      "remote: Getting sizes: 100% (11/11)\n",
      "remote: Getting sizes: 100% (11/11)\n",
      "remote: Total 15 (delta 2), reused 15 (delta 2)\n"
     ]
    }
   ],
   "source": [
    "create_algorithm(algo_name)\n",
    "cloned_repo = clone_algorithm_repo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the algorithm script and the dependencies\n",
    "Let's create the algorithm script that will run when we make our requests and the dependency file that will be used when building the container for our algorithm on the Algorithmia environment.\n",
    "\n",
    "We will be creating these two files programmatically with the %%writefile macro, but you can always use another editor to edit and save them later when you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../algorithmia_repo/xgboost_basic_sentiment_analysis/src/xgboost_basic_sentiment_analysis.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $algo_script_path\n",
    "import Algorithmia\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "model_path = \"data://asli/xgboost_demo/musicalreviews_xgb_model.pkl\"\n",
    "client = Algorithmia.client()\n",
    "model_file = client.file(model_path).getFile().name\n",
    "loaded_xgb = joblib.load(model_file)\n",
    "\n",
    "# API calls will begin at the apply() method, with the request body passed as 'input'\n",
    "# For more details, see algorithmia.com/developers/algorithm-development/languages\n",
    "def apply(input):\n",
    "    series_input = pd.Series([input])\n",
    "    result = loaded_xgb.predict(series_input)\n",
    "    # Returning the first element of the list, as we'll be taking a single input for our demo purposes\n",
    "    # As you'll see while building the model: 0->negative, 1->positive\n",
    "    return {\"sentiment\": result.tolist()[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../algorithmia_repo/xgboost_basic_sentiment_analysis/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $dependency_file_path\n",
    "algorithmia>=1.0.0,<2.0\n",
    "scikit-learn\n",
    "pandas\n",
    "numpy\n",
    "joblib\n",
    "xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding these files to git, commiting and pushing\n",
    "Now we're ready to upload our changes to our remote repo on Algorithmia and our algorithm will be built on the Algorithmia servers and get ready to accept our requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 9, done.\n",
      "Counting objects:  11% (1/9)\n",
      "Counting objects:  22% (2/9)\n",
      "Counting objects:  33% (3/9)\n",
      "Counting objects:  44% (4/9)\n",
      "Counting objects:  55% (5/9)\n",
      "Counting objects:  66% (6/9)\n",
      "Counting objects:  77% (7/9)\n",
      "Counting objects:  88% (8/9)\n",
      "Counting objects: 100% (9/9)\n",
      "Counting objects: 100% (9/9), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects:  20% (1/5)\n",
      "Compressing objects:  40% (2/5)\n",
      "Compressing objects:  60% (3/5)\n",
      "Compressing objects:  80% (4/5)\n",
      "Compressing objects: 100% (5/5)\n",
      "Compressing objects: 100% (5/5), done.\n",
      "Writing objects:  20% (1/5)\n",
      "Writing objects:  40% (2/5)\n",
      "Writing objects:  60% (3/5)\n",
      "Writing objects:  80% (4/5)\n",
      "Writing objects: 100% (5/5)\n",
      "Writing objects: 100% (5/5), 466 bytes | 466.00 KiB/s, done.\n",
      "Total 5 (delta 3), reused 0 (delta 0)\n",
      "remote: Resolving deltas:  33% (1/3)\n",
      "remote: Resolving deltas:  66% (2/3)\n",
      "remote: Resolving deltas: 100% (3/3)\n",
      "remote: Resolving deltas: 100% (3/3)\n",
      "remote: Updating references: 100% (1/1)\n",
      "remote: Updating references: 100% (1/1)\n",
      "remote:\n",
      "remote: Build successful for algo://asli/xgboost_basic_sentiment_analysis/2b704e190233020c724abf8fa2a411a5e44b9d4a\n",
      "remote:\n"
     ]
    }
   ],
   "source": [
    "files = [\"src/{}.py\".format(algo_name), \"requirements.txt\"]\n",
    "cloned_repo.index.add(files)\n",
    "cloned_repo.index.commit(\"Add algorithm files\")\n",
    "\n",
    "origin = cloned_repo.remote(name='origin')\n",
    "_ = origin.push(progress=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading the model to Algorithmia\n",
    "Let's also write the function take our saved model from its local path and put it on a data container on Algorithmia. As you'll remember, our algorithm script will be looking for the model to load at this data path.\n",
    "\n",
    "We will call this function once we're happy with our model, that we'll develop soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_model_to_algorithmia(local_path, model_name):\n",
    "    algorithmia_data_path = \"data://asli/xgboost_demo\"\n",
    "    if not algo_client.dir(algorithmia_data_path).exists():\n",
    "        algo_client.dir(algorithmia_data_path).create()\n",
    "    algorithmia_path = \"{}/{}\".format(algorithmia_data_path, model_name)\n",
    "    result = algo_client.file(algorithmia_path).putFile(local_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the algorithm\n",
    "Finally, let's write the function to call our algorithm. Again, we will use this function once our model is uploaded and we're ready to make the requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retry import retry\n",
    "# Call algorithm until the algo hash endpoint becomes available, up to 10 seconds\n",
    "@retry(AlgorithmException, tries=10, delay=1)\n",
    "def get_review_sentiment(input):\n",
    "    latest_algo_hash = algo_client.algo(algo_namespace).info().version_info.git_hash\n",
    "    algo = algo_client.algo(\"{}/{}\".format(algo_namespace, latest_algo_hash))\n",
    "    algo.set_options(timeout=60, stdout=False)\n",
    "    algo_pipe_result = algo.pipe(input)\n",
    "    print(algo_pipe_result.metadata)\n",
    "    return algo_pipe_result.result[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the XGBoost model\n",
    "Now it's time to build our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler  # for scaling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training data\n",
    "Let's load our training data, take a look at a few rows and one of the review texts in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2IBPI20UZIR0U</td>\n",
       "      <td>1384719342</td>\n",
       "      <td>cassandra tu \"Yeah, well, that's just like, u...</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Not much to write about here, but it does exac...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>good</td>\n",
       "      <td>1393545600</td>\n",
       "      <td>02 28, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A14VAT5EAX3D9S</td>\n",
       "      <td>1384719342</td>\n",
       "      <td>Jake</td>\n",
       "      <td>[13, 14]</td>\n",
       "      <td>The product does exactly as it should and is q...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Jake</td>\n",
       "      <td>1363392000</td>\n",
       "      <td>03 16, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A195EZSQDW3E21</td>\n",
       "      <td>1384719342</td>\n",
       "      <td>Rick Bennette \"Rick Bennette\"</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>The primary job of this device is to block the...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>It Does The Job Well</td>\n",
       "      <td>1377648000</td>\n",
       "      <td>08 28, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2C00NNG1ZQQG2</td>\n",
       "      <td>1384719342</td>\n",
       "      <td>RustyBill \"Sunday Rocker\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Nice windscreen protects my MXL mic and preven...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GOOD WINDSCREEN FOR THE MONEY</td>\n",
       "      <td>1392336000</td>\n",
       "      <td>02 14, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A94QU4C90B1AX</td>\n",
       "      <td>1384719342</td>\n",
       "      <td>SEAN MASLANKA</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This pop filter is great. It looks and perform...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>No more pops when I record my vocals.</td>\n",
       "      <td>1392940800</td>\n",
       "      <td>02 21, 2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  \\\n",
       "0  A2IBPI20UZIR0U  1384719342   \n",
       "1  A14VAT5EAX3D9S  1384719342   \n",
       "2  A195EZSQDW3E21  1384719342   \n",
       "3  A2C00NNG1ZQQG2  1384719342   \n",
       "4   A94QU4C90B1AX  1384719342   \n",
       "\n",
       "                                       reviewerName   helpful  \\\n",
       "0  cassandra tu \"Yeah, well, that's just like, u...    [0, 0]   \n",
       "1                                              Jake  [13, 14]   \n",
       "2                     Rick Bennette \"Rick Bennette\"    [1, 1]   \n",
       "3                         RustyBill \"Sunday Rocker\"    [0, 0]   \n",
       "4                                     SEAN MASLANKA    [0, 0]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Not much to write about here, but it does exac...      5.0   \n",
       "1  The product does exactly as it should and is q...      5.0   \n",
       "2  The primary job of this device is to block the...      5.0   \n",
       "3  Nice windscreen protects my MXL mic and preven...      5.0   \n",
       "4  This pop filter is great. It looks and perform...      5.0   \n",
       "\n",
       "                                 summary  unixReviewTime   reviewTime  \n",
       "0                                   good      1393545600  02 28, 2014  \n",
       "1                                   Jake      1363392000  03 16, 2013  \n",
       "2                   It Does The Job Well      1377648000  08 28, 2013  \n",
       "3          GOOD WINDSCREEN FOR THE MONEY      1392336000  02 14, 2014  \n",
       "4  No more pops when I record my vocals.      1392940800  02 21, 2014  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/amazon_musical_reviews/Musical_instruments_reviews.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The product does exactly as it should and is quite affordable.I did not realized it was double screened until it arrived, so it was even better than I had expected.As an added bonus, one of the screens carries a small hint of the smell of an old grape candy I used to buy, so for reminiscent's sake, I cannot stop putting the pop filter next to my nose and smelling it after recording. :DIf you needed a pop filter, this will work just as well as the expensive ones, and it may even come with a pleasing aroma like mine did!Buy this product! :]\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"reviewText\"].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Time to process our texts! Basically, we'll:\n",
    "- Remove the English stopwords\n",
    "- Remove punctuations\n",
    "- Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def threshold_ratings(data):\n",
    "    def threshold_overall_rating(rating):\n",
    "        return 0 if int(rating)<=3 else 1\n",
    "    data[\"overall\"] = data[\"overall\"].apply(threshold_overall_rating)\n",
    "\n",
    "def remove_stopwords_punctuation(data):\n",
    "    data[\"review\"] = data[\"reviewText\"] + data[\"summary\"]\n",
    "\n",
    "    puncs = list(punctuation)\n",
    "    stops = stopwords.words(\"english\")\n",
    "\n",
    "    def remove_stopwords_in_str(input_str):\n",
    "        filtered = [char for char in str(input_str).split() if char not in stops]\n",
    "        return ' '.join(filtered)\n",
    "\n",
    "    def remove_punc_in_str(input_str):\n",
    "        filtered = [char for char in input_str if char not in puncs]\n",
    "        return ''.join(filtered)\n",
    "\n",
    "    def remove_stopwords_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_stopwords_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    def remove_punc_in_series(input_series):\n",
    "        text_clean = []\n",
    "        for i in range(len(input_series)):\n",
    "            text_clean.append(remove_punc_in_str(input_series[i]))\n",
    "        return text_clean\n",
    "\n",
    "    data[\"review\"] = remove_stopwords_in_series(data[\"review\"].str.lower())\n",
    "    data[\"review\"] = remove_punc_in_series(data[\"review\"].str.lower())\n",
    "\n",
    "def drop_unused_colums(data):\n",
    "    data.drop(['reviewerID', 'asin', 'reviewerName', 'helpful', 'unixReviewTime', 'reviewTime', \"reviewText\", \"summary\"], axis=1, inplace=True)\n",
    "\n",
    "def preprocess_reviews(data):\n",
    "    remove_stopwords_punctuation(data)\n",
    "    threshold_ratings(data)\n",
    "    drop_unused_colums(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>much write here exactly supposed to filters po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>product exactly quite affordablei realized dou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>primary job device block breath would otherwis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>nice windscreen protects mxl mic prevents pops...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>pop filter great looks performs like studio fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall                                             review\n",
       "0        1  much write here exactly supposed to filters po...\n",
       "1        1  product exactly quite affordablei realized dou...\n",
       "2        1  primary job device block breath would otherwis...\n",
       "3        1  nice windscreen protects mxl mic prevents pops...\n",
       "4        1  pop filter great looks performs like studio fi..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_reviews(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split our training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "X = data[\"review\"]\n",
    "y = data[\"overall\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini randomized search\n",
    "Let's set up a very basic cross-validated randomized search over parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"max_depth\": range(9,12), \"min_child_weight\": range(5,8)}\n",
    "rand_search_cv = RandomizedSearchCV(XGBClassifier(), param_distributions=params, n_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to vectorize, transform and fit\n",
    "Time to vectorize our data, transform it and then fit our model to it.\n",
    "To be able to feed the text data as numeric values to our model, we will first convert our texts into a matrix of token counts using a CountVectorizer. Then we will convert the count matrix to a normalized tf-idf (term-frequency times inverse document-frequency) representation. Using this transformer, we will be scaling down the impact of tokens that occur very frequently, because they convey less information to us. On the contrary, we will be scaling up the impact of the tokens that occur in a small fraction of the training data because they are more informative to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('model',\n",
       "                 RandomizedSearchCV(estimator=XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            importance_type='gain',\n",
       "                                                            interaction_constraints=None,\n",
       "                                                            learning_rate=None,\n",
       "                                                            max_delta_step=None,\n",
       "                                                            max_depth=None,\n",
       "                                                            min_child_weight=None,\n",
       "                                                            missing=nan,\n",
       "                                                            monotone_constraints=None,\n",
       "                                                            n_estimators=100,\n",
       "                                                            n_jobs=None,\n",
       "                                                            num_parallel_tree=None,\n",
       "                                                            random_state=None,\n",
       "                                                            reg_alpha=None,\n",
       "                                                            reg_lambda=None,\n",
       "                                                            scale_pos_weight=None,\n",
       "                                                            subsample=None,\n",
       "                                                            tree_method=None,\n",
       "                                                            validate_parameters=None,\n",
       "                                                            verbosity=None),\n",
       "                                    n_iter=5,\n",
       "                                    param_distributions={'max_depth': range(9, 12),\n",
       "                                                         'min_child_weight': range(5, 8)}))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model', rand_search_cv)\n",
    "])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 89.14\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "print(f\"Model Accuracy: {round(acc * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "Once we're happy with our model's accuracy, let's save it locally first and then take it from there and upload to Algorithmia.\n",
    "For the Algorithmia upload, we will use our previously defined function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"musicalreviews_xgb_model.pkl\"\n",
    "local_path = f\"model/{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(model, local_path, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "algorithmia_data_path = \"data://asli/xgboost_demo\"\n",
    "upload_model_to_algorithmia(local_path, remote_data_source,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to test end to end!\n",
    "Now we are up and ready and we have a perfectly scalable algorithm on Algorithmia, waiting for its visitors! Let's test it with one positive and one negative text and see how well it does. \n",
    "To send the request to our algorithm, we will use our previously defined function and give it a string input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata(content_type='json',duration=0.020603229,stdout=None)\n",
      "Sentiment for the given text is: 0\n"
     ]
    }
   ],
   "source": [
    "pos_test_input = \"It doesn't work quite as expected. Not worth your money!\"\n",
    "sentiment = get_review_sentiment(pos_test_input)\n",
    "print(\"Sentiment for the given text is: {}\".format(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata(content_type='json',duration=0.018659969,stdout=None)\n",
      "Sentiment for the given text is: 1\n"
     ]
    }
   ],
   "source": [
    "neg_test_input = \"I am glad that I bought this. It works great!\"\n",
    "sentiment = get_review_sentiment(neg_test_input)\n",
    "print(\"Sentiment for the given text is: {}\".format(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
